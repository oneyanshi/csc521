#python-quirk

##General List
- [x] heavy lifting for parsing
- [ ] also for interpreter (what is happening)
- [ ] update readme.md as you go and w/ explanations of functions
- [ ] at least three test cases
- [x] rename files if needed
- [x] keep only necessary files within repository
- [x] add quirk-grammar folder later for easier reference

##How to Use:
Please download/clone the repository onto your desktop or laptop. From there, ensure you
are able to use Python 2.7+ (it is coded with 3.0 syntax). Within the terminal, you should
take your Quirk file and do the following:

`python lexer.py < app.q | python parser.py | python interpreter.py > output`

where `app.q` is your Quirk file. The output of the sequence should be the execution within app.q.

So if app.q's contents contain only the code `print 4` what should result is the number 4 being printed
on the terminal.

##lexer.py
### Lexer Grocery List
- [ ] fix the bugs 

lexer.py reads in a Quirk file, splits the text by white space before splitting once
more to ensure that lexemes are not stuck together. It is then tokenized based upon the Quirk tokens and specified rules within the code, i.e. what characters/letters/words are reversed for Quirk, regex, etc.

An example of what lexer.py should be doing:

Input: `var q = 2`

Output: `["VAR", "IDENT:q", "ASSIGN", "NUMBER:2"]`

This standard output then becomes the standard input for parser.py.

To solely test lexer.py, type the following in terminal:
`python lexer.py < app.q`

where `app.q` is the quirk file that you would like to lex.

##parser.py
> Interestingly enough, the parser.py will read in all of the tokens but the program is not fully parsed through if there's a second line? Or even more than one line in the program ...

The parser's standard input takes in a stream of tokens from the lexer (I have left within the program test cases/test tokens for debugging purposes). The parser is responsible for building a parser tree via an algorithm that works iteratively. In other words, functions will call on other functions within itself, thus cutting back on work and code. Thanks to Professor Josh McCoy's base, the parser can be relatively easy to understand.

The tree is built based on Quirk's grammar. Each function represents a grammar rule. For example:

`<Program> -> <Statement> <Program> | <Statement>`

is its own function, as seen within line 40 of the code. We are then able to break it down and call upon the Statement() function within the Program() function and so on. In each rule, we test for the success of the query and then continue to move down the tree based upon the functions called.

If we continued, `<Statement>` would result in the Statement() function being called, which thus breaks off into three possibilities: `<FunctionDeclaration>`, `Assignment`, or `Print`. We would work from the bottom up before terminating once we reached the EOF statement in the token stream.

### Return values
Each function will return a boolean value that is `True` if the subtree corresponds to the grammar found and `False` if it isn't. The function will also return the position into the list of tokens where the last grammar function left off. And, third, the function will return the parse tree that was generated by the grammar function. It is overall appended to the "larger" tree.


##interpreter.py
The interpreter takes in the parser's output, the tree, and then executes the code. This is where the code `print 4` would actually be executed after being ran through the lexer and the parser. The output of the interpreter should be the code's output.
